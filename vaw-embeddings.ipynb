{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a440a6-0157-4aaa-8cec-a79e2d5a5eb9",
   "metadata": {},
   "source": [
    "# Understanding Visual Concepts\n",
    "\n",
    "This notebook contains code for understanding and predicting visual attributes using various pre-trained vision models. It focuses on the Visual Attributes in the Wild (VAW) dataset and includes functionalities for extracting attributes, setting up models, computing embeddings, and preparing data for training and testing.\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdb66a6-7abf-4e8d-a958-c881de921f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/homebrew/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.9/site-packages (0.18.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.9/site-packages (4.42.4)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.9/site-packages (10.3.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (1.23.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.9/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.9/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! /opt/homebrew/Cellar/python@3.9/3.9.18_2/bin/python3.9 -m pip install torch torchvision transformers Pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a96f81-c3ad-41f2-8e9c-75457a4bea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from transformers import ViTImageProcessor, ViTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce1e71-aa97-4adb-8c76-f31a0f805ded",
   "metadata": {},
   "source": [
    "## Functions for extracting info from json files for tasks on attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f229e83b-597b-4747-a11e-a944ccfe862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(vaw_att, train_json_all):\n",
    "    total_attr = 0\n",
    "    all_attrl = []\n",
    "    for vk in vaw_att.keys():\n",
    "      total_attr += len(vaw_att[vk])\n",
    "      all_attrl.append(vaw_att[vk])\n",
    "      # print(vk, len(vaw_att[vk]), vaw_att[vk][:10])\n",
    "    all_attr = [item for sublist in all_attrl for item in sublist]\n",
    "\n",
    "    missing_attrs = []\n",
    "    for jd in train_json_all:\n",
    "        for pa in jd['positive_attributes']:\n",
    "            if pa not in all_attr:\n",
    "                missing_attrs.append(pa)\n",
    "\n",
    "    all_attrl = all_attr + missing_attrs \n",
    "    print(total_attr, len(all_attrl)) #, len(set(all_attr)))\n",
    "    return all_attrl, total_attr\n",
    "\n",
    "\n",
    "def get_ids_pa(json_all):\n",
    "    # This is very slow\n",
    "    all_ids = []\n",
    "    for jd in json_all:\n",
    "        if jd['image_id'] not in all_ids:\n",
    "            all_ids.append(jd['image_id'])\n",
    "\n",
    "    all_pa = np.zeros((len(all_ids), len(all_attrl)))\n",
    "    for jd in json_all:\n",
    "      inds = [all_attrl.index(pa) for pa in jd['positive_attributes']]\n",
    "      all_pa[all_ids.index(jd['image_id']),inds] += 1\n",
    "\n",
    "    return all_ids, all_pa\n",
    "\n",
    "def get_ids_pna(json_all):\n",
    "    # This is very slow\n",
    "    all_ids = []\n",
    "    for jd in json_all:\n",
    "        if jd['image_id'] not in all_ids:\n",
    "            all_ids.append(jd['image_id'])\n",
    "\n",
    "    all_pa = np.zeros((len(all_ids), len(all_attrl)))\n",
    "    for jd in json_all:\n",
    "      inds = [all_attrl.index(pa) for pa in jd['positive_attributes']]\n",
    "      all_pa[all_ids.index(jd['image_id']),inds] += 1\n",
    "\n",
    "    all_na = np.zeros((len(all_ids), len(all_attrl)))\n",
    "    for jd in json_all:\n",
    "      inds = [all_attrl.index(pa) for pa in jd['negative_attributes']]\n",
    "      all_na[all_ids.index(jd['image_id']),inds] += 1\n",
    "\n",
    "    return all_ids, all_pa, all_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c6cdf-cbc7-43e6-9ef9-c9882b0d6e22",
   "metadata": {},
   "source": [
    "## Load json files â€“ extract ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c8e62d-acb2-4bc0-a8b5-76e2e175796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 3010\n"
     ]
    }
   ],
   "source": [
    "# Load json files\n",
    "train_json_file1 = \"data/train_part1.json\"\n",
    "with open(train_json_file1, \"r\") as train_file1:\n",
    "    train_json1 = json.load(train_file1)\n",
    "\n",
    "train_json_file2 = \"data/train_part2.json\"\n",
    "with open(train_json_file2, \"r\") as train_file2:\n",
    "    train_json2 = json.load(train_file2)\n",
    "\n",
    "train_json_all = train_json1 + train_json2\n",
    "\n",
    "val_file_json = \"data/val.json\"\n",
    "with open(val_file_json, \"r\") as val_file:\n",
    "    val_json = json.load(val_file)\n",
    "\n",
    "attribute_file_json=\"data/attribute_types.json\"\n",
    "with open(attribute_file_json, 'r') as attribute_file:\n",
    "    vaw_att = json.load(attribute_file)\n",
    "\n",
    "# fname=\"data/attribute_index.json\"\n",
    "# with open(fname, 'r') as f:\n",
    "#    vaw_atti = json.load(f)\n",
    "\n",
    "# process json files\n",
    "all_attrl, total_attr = get_attributes(vaw_att, train_json_all)\n",
    "\n",
    "# Get ids\n",
    "all_ids, all_pa = get_ids_pa(train_json_all) \n",
    "\n",
    "all_ids_val, all_pa_val = get_ids_pa(val_json)\n",
    "\n",
    "# Save the results to files --- TO RUN -- only for first time \n",
    "with open(\"output/all_ids.json\", \"w\") as fp:\n",
    "      json.dump(all_ids, fp)\n",
    "np.save('output/all_pa',all_pa)\n",
    "\n",
    "with open(\"output/all_ids_val.json\", \"w\") as fp:\n",
    "      json.dump(all_ids_val, fp)\n",
    "np.save('output/all_pa_val',all_pa_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5181db-b127-4a1a-8ac3-7fda3a366b2d",
   "metadata": {},
   "source": [
    "## Setup / import model with hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677d8e5d-d13d-4aa4-8996-bfd4f576fd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/smarru/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/smarru/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/smarru/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/smarru/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x15b74ad90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14\")\n",
    "model.to(device)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"facebook/dino-vitb16\")\n",
    "impath = \"data/VG_100K/\"\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.norm.register_forward_hook(get_activation(\"norm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d489be6-173b-437c-a63c-7738ec96248b",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1111bcd-f36a-47bc-a4bd-ddf7382415e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_counter=500\n",
    "save_path=\"output/\"\n",
    "\n",
    "emb = []\n",
    "lactivation = []\n",
    "ids = []\n",
    "all_inputs = []\n",
    "\n",
    "for k, ui in enumerate(all_ids):\n",
    "    im = Image.open(impath + ui + \".jpg\")\n",
    "    activation = {}\n",
    "    if k > 0 and k % save_counter == 0:\n",
    "        # all_inputs = torch.stack(all_inputs).to(device)\n",
    "        # emb_all = dinov2_vitb14(all_inputs.squeeze())\n",
    "        # lactivation_all = activation[\"norm\"]\n",
    "        emb_all = torch.stack(emb)\n",
    "        lactivation_all = torch.stack(lactivation)\n",
    "        print(\"Saving at ... \" + str(k) + \" in \" + save_path)\n",
    "        torch.save(emb_all, save_path + \"emb_\" + str(k) + \".pt\")\n",
    "        torch.save(lactivation_all, save_path + \"lactivation_\" + str(k) + \".pt\")\n",
    "        with open(save_path + \"ids_\" + str(k) + \".json\", \"w\") as fp:\n",
    "            json.dump(ids, fp)\n",
    "\n",
    "        emb = []\n",
    "        lactivation = []\n",
    "        ids = []\n",
    "        all_inputs = []\n",
    "\n",
    "    if im.mode != \"L\":\n",
    "        inputs = feature_extractor(images=im, return_tensors=\"pt\")\n",
    "        # all_inputs.append(inputs[\"pixel_values\"])\n",
    "        #  emb.append(model(inputs[\"pixel_values\"]))\n",
    "        emb.append(model(inputs[\"pixel_values\"]))\n",
    "        lactivation.append(activation[\"norm\"])\n",
    "        ids.append(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d109839-2daf-4005-b933-5bc0fd6f25c3",
   "metadata": {},
   "source": [
    "## Call get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910ce1e-ea6a-419e-b8cc-5aac9d1667df",
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = \"data/VG_100K/\"\n",
    "\n",
    "compute_save_embeddings(\n",
    "    all_ids,\n",
    "    impath,\n",
    "    dinov2_vitb14,\n",
    "    feature_extractor,\n",
    "    save_counter=10,\n",
    "    save_path=\"output/test_\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f522d5-c4eb-4e1f-b72f-76b0f3a9ab3a",
   "metadata": {},
   "source": [
    "# Generate train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f9a0d-7a92-4828-8c11-fb8d42d89da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im_ids(vaw_att, all_attrl, all_ids, all_pa, vk):\n",
    "    sel_image_ids = []\n",
    "    sel_image_lab = []\n",
    "    sel_image_idsi = []\n",
    "\n",
    "    sel_attr_ids = [all_attrl.index(sid) for sid in vaw_att[vk]]\n",
    "    for k, sai in enumerate(sel_attr_ids):\n",
    "        selimi = np.nonzero(all_pa[:, sai])[0]\n",
    "        sel_image_ids.append([all_ids[si] for si in selimi])\n",
    "        sel_image_idsi.append(selimi)\n",
    "        sel_image_lab.append(k * np.ones((len(selimi))))\n",
    "\n",
    "    fsel_image_ids = [item for sublist in sel_image_ids for item in sublist]\n",
    "    fsel_image_lab = [item for sublist in sel_image_lab for item in sublist]\n",
    "    fsel_image_idsi = [item for sublist in sel_image_idsi for item in sublist]\n",
    "\n",
    "    return fsel_image_ids, fsel_image_lab, fsel_image_idsi\n",
    "\n",
    "\n",
    "def get_train_data(emb_all, fsel_image_ids, fall_emb_ids, fsel_image_lab):\n",
    "\n",
    "    # Get the index for the images from the selected attribute group\n",
    "    train_emb_ids = []\n",
    "    train_emb = []\n",
    "    train_labs = []\n",
    "    lab_len = int(np.max(np.array(fsel_image_lab))) + 1\n",
    "    for fii, lab in zip(fsel_image_ids, fsel_image_lab):\n",
    "        if fii in fall_emb_ids:\n",
    "            train_emb_ids.append(fall_emb_ids.index(fii))\n",
    "            train_emb.append(emb_all[fall_emb_ids.index(fii), :, :])\n",
    "            Y = torch.zeros((lab_len))\n",
    "            Y[int(lab)] = 1\n",
    "            train_labs.append(Y)\n",
    "\n",
    "    train_emb = torch.stack(train_emb).squeeze()\n",
    "    train_labs = torch.stack(train_labs)\n",
    "\n",
    "    return train_emb, train_labs\n",
    "\n",
    "def get_attributes(vaw_att, train_json_all):\n",
    "    total_attr = 0\n",
    "    all_attrl = []\n",
    "    for vk in vaw_att.keys():\n",
    "      total_attr += len(vaw_att[vk])\n",
    "      all_attrl.append(vaw_att[vk])\n",
    "      # print(vk, len(vaw_att[vk]), vaw_att[vk][:10])\n",
    "    all_attr = [item for sublist in all_attrl for item in sublist]\n",
    "\n",
    "    missing_attrs = []\n",
    "    for jd in train_json_all:\n",
    "        for pa in jd['positive_attributes']:\n",
    "            if pa not in all_attr:\n",
    "                missing_attrs.append(pa)\n",
    "\n",
    "    all_attrl = all_attr + missing_attrs \n",
    "    print(total_attr, len(all_attrl)) #, len(set(all_attr)))\n",
    "    return all_attrl, total_attr\n",
    "\n",
    "\n",
    "\n",
    "def get_attributes_negative(vaw_att, train_json_all):\n",
    "    total_attr = 0\n",
    "    all_attrl = []\n",
    "    for vk in vaw_att.keys():\n",
    "      total_attr += len(vaw_att[vk])\n",
    "      all_attrl.append(vaw_att[vk])\n",
    "      # print(vk, len(vaw_att[vk]), vaw_att[vk][:10])\n",
    "    all_attr = [item for sublist in all_attrl for item in sublist]\n",
    "\n",
    "    missing_attrs = []\n",
    "    for jd in train_json_all:\n",
    "        for pa in jd['negative_attributes']:\n",
    "            if pa not in all_attr:\n",
    "                missing_attrs.append(pa)\n",
    "\n",
    "    all_attrl = all_attr + missing_attrs \n",
    "    print(total_attr, len(all_attrl)) #, len(set(all_attr)))\n",
    "    return all_attrl, total_attr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a33583-c48f-4282-8d5b-75cc546119bd",
   "metadata": {},
   "source": [
    "## Combine embeddings to single variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
